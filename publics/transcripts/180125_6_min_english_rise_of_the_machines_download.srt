1
00:00:00,000 --> 00:00:03,120
This is a download from BBC Learning English.

2
00:00:03,120 --> 00:00:05,679
To find out more, visit our website.

3
00:00:12,679 --> 00:00:14,720
Hello and welcome to 6 Minute English.

4
00:00:14,720 --> 00:00:17,160
I'm Dan, and joining me today is Neil Heinew.

5
00:00:17,160 --> 00:00:18,160
Hi, Dan.

6
00:00:18,160 --> 00:00:20,559
What's with the protective gear in helmet?

7
00:00:20,559 --> 00:00:24,240
I'm just getting ready for the inevitable rise of the machines.

8
00:00:24,240 --> 00:00:29,440
That's the takeover of the world by artificial intelligence, or AI,

9
00:00:29,440 --> 00:00:32,280
which some people predict will happen.

10
00:00:32,280 --> 00:00:35,520
Inevitable means cannot be avoided or stopped.

11
00:00:35,520 --> 00:00:38,079
Rise of the machines, what do you mean?

12
00:00:38,079 --> 00:00:40,560
It's our topic in this 6 Minute English.

13
00:00:40,560 --> 00:00:44,320
We'll be talking about that, giving you six related pieces of vocabulary,

14
00:00:44,320 --> 00:00:47,439
and of course, our regular quiz question.

15
00:00:47,439 --> 00:00:50,079
That's the first thing you've said that makes any sense.

16
00:00:50,079 --> 00:00:51,280
What's the question?

17
00:00:51,280 --> 00:00:57,120
The word robot, as we use it today, was first used in a 1920s check play,

18
00:00:57,119 --> 00:00:59,599
Rossum's Universal Robots.

19
00:00:59,599 --> 00:01:03,559
But before this, what was its original meaning?

20
00:01:03,559 --> 00:01:08,759
A. Forced labour, B. Metal man, or C. Heartless thing.

21
00:01:08,759 --> 00:01:12,280
I will go for A. Forced labour.

22
00:01:12,280 --> 00:01:15,640
We'll find out if you were right or not later in the show.

23
00:01:15,640 --> 00:01:16,560
OK, Dan.

24
00:01:16,560 --> 00:01:17,879
Tell me what's going on.

25
00:01:17,879 --> 00:01:23,519
I saw a news article written by BBC Technology Correspondent Rory Ketlin Jones

26
00:01:23,560 --> 00:01:27,920
about the recent CES technology show in Las Vegas.

27
00:01:27,920 --> 00:01:32,159
He interviewed David Hansen, founder of Hansen Robotics,

28
00:01:32,159 --> 00:01:36,479
who said it was his ambition to achieve an AI

29
00:01:36,479 --> 00:01:40,759
that can beat humans at any intellectual task.

30
00:01:40,759 --> 00:01:42,399
Surely it's a good thing.

31
00:01:42,399 --> 00:01:47,439
Better AI and Robotics could take over many of the jobs that we don't want to do,

32
00:01:47,439 --> 00:01:52,319
or that are so important to get 100% right, like air traffic control.

33
00:01:52,319 --> 00:01:54,199
We'd never have another plane crash.

34
00:01:54,199 --> 00:01:57,319
It would be infallible, because it would be so clever.

35
00:01:57,319 --> 00:01:59,719
Infallible means never failing.

36
00:01:59,719 --> 00:02:01,319
And that's what bothers me.

37
00:02:01,319 --> 00:02:04,759
What happens when its intelligence surpasses hours?

38
00:02:04,759 --> 00:02:07,399
Why should it do what we want it to do?

39
00:02:07,399 --> 00:02:10,519
To surpass something is to do or be better than it.

40
00:02:10,519 --> 00:02:12,439
Dan, you've been watching too many movies.

41
00:02:12,439 --> 00:02:15,519
Robot's fighting humanity is a popular theme.

42
00:02:15,519 --> 00:02:16,519
Guess what?

43
00:02:16,519 --> 00:02:18,240
Humanity often wins.

44
00:02:18,240 --> 00:02:21,639
And besides, we would program the computer to be benevolent.

45
00:02:21,639 --> 00:02:24,279
Benevolent means kind and helpful.

46
00:02:24,279 --> 00:02:25,559
But that's just it.

47
00:02:25,559 --> 00:02:30,119
Once the intelligence becomes sentient, or able to think for itself,

48
00:02:30,119 --> 00:02:31,879
who knows what it will do?

49
00:02:31,879 --> 00:02:34,359
We humans are not exactly perfect, you know.

50
00:02:34,359 --> 00:02:39,079
What happens if it decides that it's better than us and wants us out of the way?

51
00:02:39,079 --> 00:02:41,919
Don't worry, Asimov thought of that.

52
00:02:41,919 --> 00:02:45,039
Isaac Asimov was an American science fiction writer

53
00:02:45,039 --> 00:02:47,759
who, among other things, wrote about robots.

54
00:02:47,759 --> 00:02:51,759
He came up with three laws that every robot would have to follow

55
00:02:51,759 --> 00:02:54,000
to stop it from acting against humanity.

56
00:02:54,000 --> 00:02:55,519
So we're safe.

57
00:02:55,519 --> 00:02:57,039
I'm not so sure.

58
00:02:57,039 --> 00:02:59,519
A sentient robot could make up its own mind

59
00:02:59,519 --> 00:03:01,840
about how to interpret the laws.

60
00:03:01,840 --> 00:03:05,479
For example, imagine if we created an AI system

61
00:03:05,479 --> 00:03:07,359
to protect all of humanity.

62
00:03:07,359 --> 00:03:09,319
Well, that's great. No more war.

63
00:03:09,319 --> 00:03:11,239
No more murder, no more fighting.

64
00:03:11,239 --> 00:03:14,000
Do you really think that humans can stop fighting?

65
00:03:14,000 --> 00:03:16,719
What if the AI decides that the only way

66
00:03:16,719 --> 00:03:20,560
to stop us from hurting ourselves and each other

67
00:03:20,560 --> 00:03:23,280
is to control everything we do?

68
00:03:23,280 --> 00:03:26,680
So it takes over to protect us.

69
00:03:26,680 --> 00:03:30,360
Then we would lose our freedom to a thing that we created

70
00:03:30,360 --> 00:03:34,319
that is infallible and more intelligent than we are.

71
00:03:34,319 --> 00:03:35,919
That's the end, Neil.

72
00:03:35,919 --> 00:03:37,879
I think that's a little far-fetched,

73
00:03:37,879 --> 00:03:39,439
which means difficult to believe.

74
00:03:39,439 --> 00:03:41,240
I'm sure others don't think that way.

75
00:03:41,240 --> 00:03:42,039
OK.

76
00:03:42,039 --> 00:03:44,360
Let's hear what the learning English teams say

77
00:03:44,360 --> 00:03:47,880
when I ask them if they're worried that AI and robots

78
00:03:47,880 --> 00:03:50,680
could take over the world.

79
00:03:50,680 --> 00:03:53,760
Well, it's possible, but unlikely.

80
00:03:53,760 --> 00:03:55,960
They'll come a point where our technology will be limited

81
00:03:55,960 --> 00:04:00,240
and probably before real AI is achieved.

82
00:04:00,240 --> 00:04:02,320
Never in a million years.

83
00:04:02,320 --> 00:04:05,720
First of all, we'd program them so that they couldn't.

84
00:04:05,720 --> 00:04:08,520
And secondly, we'd beat them anyway.

85
00:04:08,520 --> 00:04:12,040
Haven't you ever seen a movie?

86
00:04:12,040 --> 00:04:14,040
I totally think you could happen.

87
00:04:14,039 --> 00:04:16,599
We only have to make a robot that's smart enough

88
00:04:16,599 --> 00:04:18,439
to start thinking for itself.

89
00:04:18,439 --> 00:04:21,719
After that, who knows what it might do?

90
00:04:21,719 --> 00:04:23,439
A mixed bag of opinions there, Dan.

91
00:04:23,439 --> 00:04:25,560
It seems you aren't alone.

92
00:04:25,560 --> 00:04:26,360
Nope.

93
00:04:26,360 --> 00:04:29,759
But I don't exactly have an army of support, either.

94
00:04:29,759 --> 00:04:31,480
I guess we'll just have to wait and see.

95
00:04:31,480 --> 00:04:32,519
Speak for yourself.

96
00:04:32,519 --> 00:04:35,279
I've waited long enough for our quiz question, that is.

97
00:04:35,279 --> 00:04:35,879
Oh, yeah.

98
00:04:35,879 --> 00:04:39,279
I asked you what the original meaning of the word robot was

99
00:04:39,279 --> 00:04:42,159
before it was used in its modern form.

100
00:04:42,680 --> 00:04:44,920
Shall we take a look at the vocabulary there?

101
00:04:44,920 --> 00:04:45,760
Okay.

102
00:04:45,760 --> 00:04:47,600
First, we had inevitable.

103
00:04:47,600 --> 00:04:51,800
If something is inevitable, then it cannot be avoided or stopped.

104
00:04:51,800 --> 00:04:53,800
Can you think of something inevitable, Neil?

105
00:04:53,800 --> 00:04:57,880
It is inevitable that one day the sun will stop burning.

106
00:04:57,880 --> 00:05:01,879
Then we had infallible, which means never failing.

107
00:05:01,879 --> 00:05:03,480
Give us an example, Dan.

108
00:05:03,480 --> 00:05:06,080
The vaccine for the vaccine is inevitable.

109
00:05:06,080 --> 00:05:07,080
It's inevitable.

110
00:05:07,080 --> 00:05:08,080
It's inevitable.

111
00:05:08,080 --> 00:05:09,080
It's inevitable.

112
00:05:09,080 --> 00:05:12,080
It's inevitable that one day the sun will stop burning.

113
00:05:12,079 --> 00:05:15,560
After smallpox is infallible, the natural spread of that disease

114
00:05:15,560 --> 00:05:17,560
has been completely stopped.

115
00:05:17,560 --> 00:05:19,879
After that was surpasses.

116
00:05:19,879 --> 00:05:22,399
If something surpasses something else,

117
00:05:22,399 --> 00:05:24,360
then it becomes better than it.

118
00:05:24,360 --> 00:05:26,959
Many parents across the world hope that their children

119
00:05:26,959 --> 00:05:30,719
will surpass them in wealth, status, or achievement.

120
00:05:30,719 --> 00:05:33,959
After that, we heard benevolent, which means kind and helpful.

121
00:05:33,959 --> 00:05:36,599
Name a person famous for being benevolent, Dan.

122
00:05:36,599 --> 00:05:37,399
Hmm.

123
00:05:37,399 --> 00:05:40,360
Father Christmas is a benevolent character.

124
00:05:40,360 --> 00:05:42,480
After that, we heard sentient.

125
00:05:42,480 --> 00:05:45,879
If something is sentient, it is able to think for itself.

126
00:05:45,879 --> 00:05:51,000
Indeed, many people wonder about the possibility of sentient life on other planets.

127
00:05:51,000 --> 00:05:54,720
Finally, we heard far-fetched, which means difficult to believe.

128
00:05:54,720 --> 00:05:57,879
Like that far-fetched story you told me the other day about being late,

129
00:05:57,879 --> 00:05:59,160
because of a dragon, Dan.

130
00:05:59,160 --> 00:06:00,600
I swear it was real.

131
00:06:00,600 --> 00:06:02,680
It had a big sharp teeth and everything.

132
00:06:02,680 --> 00:06:03,879
Yeah, yeah, yeah.

133
00:06:03,879 --> 00:06:06,040
And that's the end of this six-minute English.

134
00:06:06,040 --> 00:06:09,280
Don't forget to check out our Facebook, Twitter, and YouTube pages.

135
00:06:09,279 --> 00:06:10,279
See you next time.

136
00:06:10,279 --> 00:06:11,279
Bye.

137
00:06:11,279 --> 00:06:12,279
Bye.

138
00:06:12,279 --> 00:06:14,079
Six-minute English.

139
00:06:14,079 --> 00:06:16,279
From BBCLearningEnglish.com.

