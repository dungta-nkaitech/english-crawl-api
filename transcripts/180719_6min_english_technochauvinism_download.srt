1
00:00:00,000 --> 00:00:03,120
This is a download from BBC Learning English.

2
00:00:03,120 --> 00:00:05,679
To find out more, visit our website.

3
00:00:05,679 --> 00:00:08,240
6M8English

4
00:00:08,240 --> 00:00:11,120
from BBCLearningEnglish.com

5
00:00:12,560 --> 00:00:15,919
Hello and welcome to 6M8English. I'm Catherine.

6
00:00:15,919 --> 00:00:17,039
And hello, I'm Rob.

7
00:00:17,039 --> 00:00:19,519
Today we have another technology topic.

8
00:00:19,519 --> 00:00:21,359
Oh, good, I love technology.

9
00:00:21,359 --> 00:00:25,440
It makes things easier. It's fast and means I can have gadgets.

10
00:00:25,440 --> 00:00:29,760
Do you think that technology can actually do things better than humans?

11
00:00:29,760 --> 00:00:35,039
For some things, yes. I think cars that drive themselves will be safer than humans,

12
00:00:35,039 --> 00:00:38,480
but that will take away some of the pleasure of driving.

13
00:00:38,480 --> 00:00:41,039
So I guess it depends on what you mean by better.

14
00:00:41,039 --> 00:00:45,840
Good point, Rob. And that actually ties in very closely with today's topic,

15
00:00:45,840 --> 00:00:48,320
which is techno-shovenism.

16
00:00:48,320 --> 00:00:49,600
Hmm, what's that?

17
00:00:49,600 --> 00:00:51,040
We'll find out shortly, Rob.

18
00:00:51,040 --> 00:00:54,240
But before we do, today's quiz question.

19
00:00:54,240 --> 00:00:59,520
Artificial intelligence or AI is an area of computer science that develops

20
00:00:59,520 --> 00:01:03,760
the ability of computers to learn to do things like solve problems

21
00:01:03,760 --> 00:01:05,920
or drive cars without crashing.

22
00:01:05,920 --> 00:01:11,200
But in what decade was the term artificial intelligence coined?

23
00:01:11,200 --> 00:01:17,680
Was it A, the 1940s, B, the 1950s, or C, the 1960s?

24
00:01:17,680 --> 00:01:20,159
I think it's quite a new expression,

25
00:01:20,159 --> 00:01:22,640
so I'll go for C, the 1960s.

26
00:01:22,640 --> 00:01:26,560
Good luck with that, Rob. And we'll give you the answer later in the programme.

27
00:01:26,560 --> 00:01:30,240
Now, let's get back to our topic of techno-shovenism.

28
00:01:30,240 --> 00:01:32,159
I know what a showvinist is.

29
00:01:32,159 --> 00:01:37,280
It's someone who thinks that their country or race or sex is better than others.

30
00:01:37,280 --> 00:01:39,200
But how does this relate to technology?

31
00:01:39,200 --> 00:01:40,719
We're about to find out.

32
00:01:40,719 --> 00:01:45,280
Meredith Bussard is professor of journalism at New York University,

33
00:01:45,280 --> 00:01:49,439
and she's written a book called Artificial Uneintelligence.

34
00:01:49,439 --> 00:01:54,320
She appeared on the BBC Radio 4 programme more or less to talk about it.

35
00:01:54,319 --> 00:01:58,319
Listen carefully and find out her definition of techno-shovenism.

36
00:01:59,039 --> 00:02:05,439
Techno-shovenism is the idea that technology is always the highest and best solution.

37
00:02:05,439 --> 00:02:10,719
So somehow over the past couple of decades, we got into the habit of thinking

38
00:02:10,719 --> 00:02:18,799
that doing something with a computer is always the best and most objective way to do something.

39
00:02:18,799 --> 00:02:20,400
And that's simply not true.

40
00:02:20,400 --> 00:02:25,280
Computers are not objective. They are her proxies for the people who make them.

41
00:02:26,080 --> 00:02:30,240
What is Meredith Bussard's definition of techno-shovenism?

42
00:02:30,240 --> 00:02:35,280
It's this idea that using technology is better than not using technology.

43
00:02:35,280 --> 00:02:39,759
She says that we have this idea that a computer is objective.

44
00:02:39,759 --> 00:02:43,599
Something that's objective is neutral. It doesn't have an opinion.

45
00:02:43,599 --> 00:02:48,319
It's fair and it's unbiased. So it's the opposite of being a showvinist.

46
00:02:48,400 --> 00:02:52,079
But Meredith Bussard says this is not true.

47
00:02:52,079 --> 00:02:55,359
She argues that computers are not objective.

48
00:02:55,359 --> 00:02:58,560
They are proxies for the people that make them.

49
00:02:58,560 --> 00:03:02,639
You might know the word proxy when you're using your computer in one country

50
00:03:02,639 --> 00:03:06,400
and want to look at something that is only available in a different country.

51
00:03:06,400 --> 00:03:09,759
You can use a piece of software called a proxy to do that.

52
00:03:09,759 --> 00:03:16,799
But a proxy is also a person or a thing that carries out your wishes and your instructions for you.

53
00:03:17,439 --> 00:03:23,520
So computers are only as smart or as objective as the people that program them.

54
00:03:24,160 --> 00:03:27,600
Computers are proxies for their programmers.

55
00:03:27,600 --> 00:03:33,520
Bussard says that believing too much in artificial intelligence can make the word worse.

56
00:03:34,080 --> 00:03:40,480
Let's hear a bit more. This time find out what serious problems in society does she think

57
00:03:40,639 --> 00:03:42,639
may be reflected in AI.

58
00:03:43,599 --> 00:03:48,560
It's a nuanced problem. What we have is data on the world as it is.

59
00:03:48,560 --> 00:03:53,439
And we have serious problems with racism, sexism, classism,

60
00:03:53,439 --> 00:03:58,959
ageism in the world right now. So there is no such thing as perfect data.

61
00:03:58,959 --> 00:04:05,599
We also have a problem inside the tech world where the creators of algorithms do not have

62
00:04:05,840 --> 00:04:16,079
sufficient awareness of social issues such that they can make good technology that gets us closer to a world as it should be.

63
00:04:17,040 --> 00:04:23,199
She said that society has problems with racism, sexism, classism and ageism.

64
00:04:23,199 --> 00:04:31,360
And she says it's a nuanced problem. A nuanced problem is not simple but it does have small and important areas

65
00:04:31,360 --> 00:04:34,959
which may be hard to spot but they need to be considered.

66
00:04:34,959 --> 00:04:40,239
And she also talked about algorithms used to program these technological systems.

67
00:04:40,239 --> 00:04:45,199
An algorithm is a set of instructions that computers use to perform their tasks.

68
00:04:45,199 --> 00:04:49,039
Essentially it's the rules that they use to come up with their answers.

69
00:04:49,039 --> 00:04:54,399
And Bussard believes that technology will reflect the views of those who create the algorithms.

70
00:04:54,399 --> 00:04:59,439
Next time you're using a piece of software or your favourite app, you might find yourself wondering

71
00:04:59,519 --> 00:05:06,719
if it's a useful tool or does it contain these little nuances that reflect the views of the developer?

72
00:05:06,719 --> 00:05:10,000
Right Catherine, how about the answer to this speech question then?

73
00:05:10,000 --> 00:05:14,480
I asked in which decade was the term artificial intelligence kind?

74
00:05:14,480 --> 00:05:17,279
Was it the 40s, the 50s or the 60s?

75
00:05:17,279 --> 00:05:18,639
And I said the 60s.

76
00:05:18,639 --> 00:05:24,319
It was actually the 1950s. Never mind Rob. Let's review today's vocabulary.

77
00:05:25,040 --> 00:05:31,040
A showvinist. That's someone who believes their country, race or sex is better than any others.

78
00:05:31,040 --> 00:05:37,519
And this gives us techno-shovanism. The belief that a technological solution is always a better

79
00:05:37,519 --> 00:05:44,560
solution to a problem. Next, someone or something that is objective is neutral, fair and balanced.

80
00:05:44,560 --> 00:05:51,360
A proxy is a piece of software but also someone who does something for you on your behalf.

81
00:05:51,360 --> 00:05:56,879
A nuanced problem is a subtle one. It's not a simple case of right or wrong.

82
00:05:56,879 --> 00:06:02,160
In a nuanced problem, there are small but important things that you need to consider.

83
00:06:02,160 --> 00:06:06,480
And an algorithm is a set of software instructions for a computer system.

84
00:06:06,480 --> 00:06:08,800
Well, that's all we have time for today.

85
00:06:08,800 --> 00:06:09,920
Goodbye for now.

86
00:06:09,920 --> 00:06:10,560
Bye-bye.

