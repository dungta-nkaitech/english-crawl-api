1
00:00:00,000 --> 00:00:07,000
This is a download from BBC Learning English. To find out more, visit our website.

2
00:00:07,000 --> 00:00:15,000
6-Milk English from BBCLearningEnglish.com

3
00:00:15,000 --> 00:00:19,000
Hello, this is 6-Minute English from BBC Learning English. I'm Sam.

4
00:00:19,000 --> 00:00:20,000
And I'm Neil.

5
00:00:20,000 --> 00:00:27,000
In the autumn of 2021, something strange happened at the Google headquarters in California's Silicon Valley.

6
00:00:27,000 --> 00:00:33,000
A software engineer called Blake LeMoin was working on the Artificial Intelligence Project,

7
00:00:33,000 --> 00:00:38,000
language models for dialogue applications, or lambda for short.

8
00:00:38,000 --> 00:00:44,000
Lambda is a chatbot, a computer programme designed to have conversations with humans over the internet.

9
00:00:44,000 --> 00:00:50,000
After months talking with Lambda on topics ranging from movies to the meaning of life,

10
00:00:50,000 --> 00:00:53,000
Blake came to a surprising conclusion.

11
00:00:53,000 --> 00:00:59,000
The chatbot was an intelligent person with wishes and rights that should be respected.

12
00:00:59,000 --> 00:01:05,000
For Blake, Lambda was a Google employee, not a machine. He also called it his friend.

13
00:01:05,000 --> 00:01:12,000
Google quickly reassigned Blake from the project, announcing that his ideas were not supported by the evidence.

14
00:01:12,000 --> 00:01:14,000
But what exactly was going on?

15
00:01:14,000 --> 00:01:20,000
In this programme, we'll be discussing whether artificial intelligence is capable of consciousness.

16
00:01:21,000 --> 00:01:26,000
We'll hear from one expert who thinks AI is not as intelligent as we sometimes think,

17
00:01:26,000 --> 00:01:30,000
and as usual, we'll be learning some new vocabulary as well.

18
00:01:30,000 --> 00:01:32,000
But before that, I have a question for you, Neil.

19
00:01:32,000 --> 00:01:38,000
What happened to Blake LeMoin is strangely similar to the 2013 Hollywood movie, HER.

20
00:01:38,000 --> 00:01:44,000
Starring Joaquin Phoenix as a lonely writer who talks with his computer voiced by Scarlett Johansson.

21
00:01:44,000 --> 00:01:47,000
But what happens at the end of the movie?

22
00:01:47,000 --> 00:01:49,000
Is it A? The computer comes to life.

23
00:01:49,000 --> 00:01:52,000
B. The computer dreams about the writer.

24
00:01:52,000 --> 00:01:55,000
Or C. The writer falls in love with the computer.

25
00:01:55,000 --> 00:01:58,000
C. The writer falls in love with the computer.

26
00:01:58,000 --> 00:02:02,000
Ok, Neil, I'll reveal the answer at the end of the programme.

27
00:02:02,000 --> 00:02:14,000
Although Hollywood is full of movies about robots coming to life, Emily Bender, a professor of linguistics and computing at the University of Washington, thinks AI isn't that smart.

28
00:02:14,000 --> 00:02:20,000
She thinks the words we use to talk about technology, phrases like machine learning,

29
00:02:20,000 --> 00:02:24,000
give a false impression about what computers can and can't do.

30
00:02:24,000 --> 00:02:33,000
Here is Professor Bender discussing another misleading phrase, speech recognition, with BBC World Service programme The Inquiry.

31
00:02:33,000 --> 00:02:41,000
If you talk about automatic speech recognition, the term recognition suggests that there's something cognitive going on,

32
00:02:41,000 --> 00:02:47,000
where I think a better term would be automatic transcription, that just describes the input output relation,

33
00:02:47,000 --> 00:02:54,000
and not any theory or wishful thinking about what the computer is doing to be able to achieve that.

34
00:02:54,000 --> 00:03:01,000
Using words like recognition in relation to computers gives the idea that something cognitive is happening,

35
00:03:01,000 --> 00:03:07,000
something related to the mental processes of thinking, knowing, learning and understanding.

36
00:03:07,000 --> 00:03:12,000
But thinking and knowing are human, not machine activities.

37
00:03:12,000 --> 00:03:21,000
Professor Bender says that talking about them in connection with computers is wishful thinking, something which is unlikely to happen.

38
00:03:21,000 --> 00:03:28,000
The problem with using words in this way is that it reinforces what Professor Bender calls technical bias,

39
00:03:28,000 --> 00:03:31,000
the assumption that the computer is always right.

40
00:03:31,000 --> 00:03:39,000
When we encounter language that sounds natural, but is coming from a computer, humans can't help but imagine a mind behind the language.

41
00:03:39,000 --> 00:03:41,000
Even when there isn't one.

42
00:03:41,000 --> 00:03:46,000
In other words, we anthropomorphize computers. We treat them as if they were human.

43
00:03:46,000 --> 00:03:54,000
Here's Professor Bender again discussing this idea with Charmaine Cozier, the presenter of BBC World Services The Inquiry.

44
00:03:55,000 --> 00:04:01,000
So ism means system, anthropo means human, and morph means shape.

45
00:04:01,000 --> 00:04:06,000
And so this is a system that puts the shape of a human on something.

46
00:04:06,000 --> 00:04:08,000
And in this case, the something is a computer.

47
00:04:08,000 --> 00:04:16,000
We anthropomorphize animals all the time, but we also anthropomorphize action figures or dolls or companies.

48
00:04:16,000 --> 00:04:23,000
When we talk about companies having intentions and so on, we very much are in the habit of seeing ourselves in the world around us.

49
00:04:24,000 --> 00:04:30,000
And while we're busy seeing ourselves by signing human traits to things that are not, we risk being blindsided.

50
00:04:30,000 --> 00:04:37,000
The more fluent that text is, the more different topics it can converse on, the more chances there are to get taken in.

51
00:04:37,000 --> 00:04:49,000
If we treat computers as if they could think we might get blindsided or unpleasantly surprised, artificial intelligence works by finding patterns in massive amounts of data.

52
00:04:50,000 --> 00:04:56,000
So it can seem like we're talking with a human instead of a machine doing data analysis.

53
00:04:56,000 --> 00:05:04,000
As a result, we get taken in, were tricked or deceived into thinking we're dealing with a human or with something intelligent.

54
00:05:04,000 --> 00:05:15,000
Powerful AI can make machines appear conscious, but even tech giants like Google are years away from building computers that can dream or fall in love.

55
00:05:15,000 --> 00:05:18,000
Speaking of which, Sam, what was the answer to your question?

56
00:05:18,000 --> 00:05:21,000
I asked what happened in the 2013 movie, Her.

57
00:05:21,000 --> 00:05:26,000
Neil thought that the main character falls in love with his computer, which was the correct answer.

58
00:05:26,000 --> 00:05:34,000
Ah, okay. Right, it's time to recap the vocabulary we've learned from this program about AI, including chatbots.

59
00:05:34,000 --> 00:05:38,000
Computer programs designed to interact with humans over the internet.

60
00:05:38,000 --> 00:05:45,000
The adjective cognitive describes anything connected with the mental processes of knowing, learning and understanding.

61
00:05:46,000 --> 00:05:53,000
Wishful thinking means thinking that something which is very unlikely to happen might happen one day in the future.

62
00:05:53,000 --> 00:05:58,000
To anthropomorphize an object means to treat it as if it were human, even though it's not.

63
00:05:58,000 --> 00:06:02,000
When you're blindsided, you're surprised in a negative way.

64
00:06:02,000 --> 00:06:07,000
And finally, to get taken in by someone means to be deceived or tricked by them.

65
00:06:07,000 --> 00:06:13,000
My computer tells me that our six minutes are up. Join us again soon for now. It's goodbye from us.

66
00:06:13,000 --> 00:06:17,000
Six-minute English from the BBC.

